{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/duxy/Downloads/Study/NLP/第四期NLP项目/Project1/BaiduAutoMaster\n",
      "Creating the vocab ...\n",
      "max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 30000 total words. Last word added: 月牙\n",
      "Creating the batcher ...\n",
      "Building the model ...\n",
      "Creating the checkpoint manager\n",
      "Initializing from scratch.\n",
      "Starting the training ...\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "import os\n",
    "# BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "BASE_DIR = os.path.abspath(\"\")\n",
    "print(BASE_DIR)\n",
    "sys.path.append(BASE_DIR+\"/seq2seq_pgn_tf2\")\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "from seq2seq_pgn_tf2.train_eval_test import train, predict_result\n",
    "# from utils.log_utils import define_logger\n",
    "import pathlib\n",
    "\n",
    "NUM_SAMPLES = 82706\n",
    "# 获取项目根目录\n",
    "# root = pathlib.Path(os.path.abspath(__file__)).parent.parent\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # 模型参数\n",
    "    parser.add_argument(\"--max_enc_len\", default=200, help=\"Encoder input max sequence length\", type=int)\n",
    "    parser.add_argument(\"--max_dec_len\", default=40, help=\"Decoder input max sequence length\", type=int)\n",
    "    parser.add_argument(\"--max_dec_steps\", default=100,\n",
    "                        help=\"maximum number of words of the predicted abstract\", type=int)\n",
    "    parser.add_argument(\"--min_dec_steps\", default=30,\n",
    "                        help=\"Minimum number of words of the predicted abstract\", type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=32, help=\"batch size\", type=int)\n",
    "    parser.add_argument(\"--beam_size\", default=3,\n",
    "                        help=\"beam size for beam search decoding (must be equal to batch size in decode mode)\",\n",
    "                        type=int)\n",
    "    parser.add_argument(\"--vocab_size\", default=30000, help=\"Vocabulary size\", type=int)\n",
    "    parser.add_argument(\"--embed_size\", default=256, help=\"Words embeddings dimension\", type=int)\n",
    "    parser.add_argument(\"--enc_units\", default=256, help=\"Encoder GRU cell units number\", type=int)\n",
    "    parser.add_argument(\"--dec_units\", default=256, help=\"Decoder GRU cell units number\", type=int)\n",
    "    parser.add_argument(\"--attn_units\", default=256,\n",
    "                        help=\"[context vector, decoder state, decoder input] feedforward result dimension - \"\n",
    "                             \"this result is used to compute the attention weights\", type=int)\n",
    "    parser.add_argument(\"--learning_rate\", default=0.001, help=\"Learning rate\", type=float)\n",
    "    parser.add_argument(\"--adagrad_init_acc\", default=0.1,\n",
    "                        help=\"Adagrad optimizer initial accumulator value. Please refer to the Adagrad optimizer \"\n",
    "                             \"API documentation on tensorflow site for more details.\", type=float)\n",
    "    parser.add_argument(\"--max_grad_norm\", default=0.8, help=\"Gradient norm above which gradients must be clipped\",\n",
    "                        type=float)\n",
    "    parser.add_argument('--cov_loss_wt', default=0.5, help='Weight of coverage loss (lambda in the paper).'\n",
    "                                                           ' If zero, then no incentive to minimize coverage loss.',\n",
    "                        type=float)\n",
    "\n",
    "    # path\n",
    "    # /ckpt/checkpoint/checkpoint\n",
    "    parser.add_argument(\"--seq2seq_model_dir\", default='{}/ckpt/seq2seq'.format(BASE_DIR), help=\"Model folder\")\n",
    "    parser.add_argument(\"--pgn_model_dir\", default='{}/ckpt/pgn'.format(BASE_DIR), help=\"Model folder\")\n",
    "    parser.add_argument(\"--model_path\", help=\"Path to a specific model\", default=\"\", type=str)\n",
    "    parser.add_argument(\"--train_seg_x_dir\", default='{}/datasets/train_set.seg_x.txt'.format(BASE_DIR), help=\"train_seg_x_dir\")\n",
    "    parser.add_argument(\"--train_seg_y_dir\", default='{}/datasets/train_set.seg_y.txt'.format(BASE_DIR), help=\"train_seg_y_dir\")\n",
    "    parser.add_argument(\"--test_seg_x_dir\", default='{}/datasets/test_set.seg_x.txt'.format(BASE_DIR), help=\"test_seg_x_dir\")\n",
    "    parser.add_argument(\"--vocab_path\", default='{}/datasets/vocab.txt'.format(BASE_DIR), help=\"Vocab path\")\n",
    "    parser.add_argument(\"--word2vec_output\", default='{}/datasets/word2vec.txt'.format(BASE_DIR), help=\"Vocab path\")\n",
    "    parser.add_argument(\"--log_file\", help=\"File in which to redirect console outputs\", default=\"\", type=str)\n",
    "    parser.add_argument(\"--test_save_dir\", default='{}/datasets/'.format(BASE_DIR), help=\"test_save_dir\")\n",
    "    parser.add_argument(\"--test_x_dir\", default='{}/datasets/AutoMaster_TestSet.csv'.format(BASE_DIR), help=\"test_x_dir\")\n",
    "\n",
    "    # others\n",
    "    parser.add_argument(\"--steps_per_epoch\", default=8087, help=\"max_train_steps\", type=int)\n",
    "    parser.add_argument(\"--checkpoints_save_steps\", default=10, help=\"Save checkpoints every N steps\", type=int)\n",
    "    parser.add_argument(\"--max_steps\", default=10000, help=\"Max number of iterations\", type=int)\n",
    "    parser.add_argument(\"--num_to_test\", default=20000, help=\"Number of examples to test\", type=int)\n",
    "    parser.add_argument(\"--max_num_to_eval\", default=5, help=\"max_num_to_eval\", type=int)\n",
    "    parser.add_argument(\"--epochs\", default=20, help=\"train epochs\", type=int)\n",
    "    \n",
    "    # transformer\n",
    "    parser.add_argument('--d_model', default=512, type=int,\n",
    "                        help=\"hidden dimension of encoder/decoder\")\n",
    "    parser.add_argument('--num_blocks', default=6, type=int,\n",
    "                        help=\"number of encoder/decoder blocks\")\n",
    "    parser.add_argument('--num_heads', default=8, type=int,\n",
    "                        help=\"number of attention heads\")\n",
    "    parser.add_argument('--dff', default=2048, type=int,\n",
    "                        help=\"hidden dimension of feedforward layer\")\n",
    "    parser.add_argument('--dropout_rate', default=0.1, type=float)\n",
    "    \n",
    "    # mode\n",
    "    parser.add_argument(\"--mode\", default='train', help=\"training, eval or test options\")\n",
    "    parser.add_argument(\"--model\", default='PGN', help=\"which model to be slected\")\n",
    "    parser.add_argument(\"--pointer_gen\", default=True, help=\"training, eval or test options\")\n",
    "    parser.add_argument(\"--is_coverage\", default=True, help=\"is_coverage\")\n",
    "    parser.add_argument(\"--greedy_decode\", default=False, help=\"greedy_decoder\")\n",
    "    parser.add_argument(\"--transformer\", default=False, help=\"transformer\")\n",
    "    parser.add_argument('-f')\n",
    "    args = parser.parse_args()\n",
    "    params = vars(args)\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "    # print('grus is ', gpus)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(devices=gpus[3], device_type='GPU')\n",
    "\n",
    "    if params[\"mode\"] == \"train\":\n",
    "        params[\"steps_per_epoch\"] = NUM_SAMPLES//params[\"batch_size\"]\n",
    "        train(params)\n",
    "    \n",
    "    # elif params[\"mode\"] == \"eval\":\n",
    "    #     evaluate(params)\n",
    "\n",
    "    elif params[\"mode\"] == \"test\":\n",
    "        params[\"batch_size\"] = params[\"beam_size\"]\n",
    "        predict_result(params)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
